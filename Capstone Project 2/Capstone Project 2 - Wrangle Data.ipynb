{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'emojize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a391fe92dc5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memojize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Phd is very easy!!! :thumbs_up:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'emojize'"
     ]
    }
   ],
   "source": [
    "print(emoji.emojize('Phd is very easy!!! :thumbs_up:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Working Directory and Concatenate Multiple Datasets into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory C:\\Users\\melin\n",
      "Directory changed successfully C:\\Users\\melin\\Documents\\Springboard Data Science Career Track\\Capstone Projects\\Capstone Project 2\\COVID-19 Tweets\\CP2data\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\melin\\\\Documents\\\\Springboard Data Science Career Track\\\\Capstone Projects\\\\Capstone Project 2\\\\COVID-19 Tweets\\\\CP2data'\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "print(\"Current working directory %s\" % retval)\n",
    "\n",
    "# Now change the directory\n",
    "os.chdir(path)\n",
    "\n",
    "# Check current working directory.\n",
    "retval = os.getcwd()\n",
    "\n",
    "print(\"Directory changed successfully %s\" % retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use glob to match the pattern ‘csv’\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all files in the list\n",
    "df_tweets = pd.concat([pd.read_csv(f) for f in all_filenames ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>reply_to_status_id</th>\n",
       "      <th>reply_to_user_id</th>\n",
       "      <th>reply_to_screen_name</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>country_code</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_type</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>account_lang</th>\n",
       "      <th>account_created_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244051646071611394</td>\n",
       "      <td>860252856829587457</td>\n",
       "      <td>2020-03-29T00:00:00Z</td>\n",
       "      <td>IMSS_SanLuis</td>\n",
       "      <td>Ante cualquier enfermedad respiratoria, no te ...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1008</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-04T22:00:38Z</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244051645039706112</td>\n",
       "      <td>1125933654943895553</td>\n",
       "      <td>2020-03-29T00:00:00Z</td>\n",
       "      <td>intrac_ccs</td>\n",
       "      <td>#ATENCIÓN En el Terminal Nuevo Circo se implem...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-08T01:21:16Z</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244051645975191557</td>\n",
       "      <td>80943559</td>\n",
       "      <td>2020-03-29T00:00:00Z</td>\n",
       "      <td>rlieving</td>\n",
       "      <td>“People are just storing up. They are staying ...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-10-08T21:06:08Z</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244051646750928897</td>\n",
       "      <td>817072420947247104</td>\n",
       "      <td>2020-03-29T00:00:00Z</td>\n",
       "      <td>Tu_IMSS_Coah</td>\n",
       "      <td>Si empezaste a trabajar, necesitas dar de alta...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1549</td>\n",
       "      <td>170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-05T18:17:00Z</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244051647032102914</td>\n",
       "      <td>788863557349670913</td>\n",
       "      <td>2020-03-29T00:00:00Z</td>\n",
       "      <td>Tabasco_IMSS</td>\n",
       "      <td>Una sociedad informada está mejor preparada an...</td>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>868</td>\n",
       "      <td>125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-10-19T22:05:03Z</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id              user_id            created_at  \\\n",
       "0  1244051646071611394   860252856829587457  2020-03-29T00:00:00Z   \n",
       "1  1244051645039706112  1125933654943895553  2020-03-29T00:00:00Z   \n",
       "2  1244051645975191557             80943559  2020-03-29T00:00:00Z   \n",
       "3  1244051646750928897   817072420947247104  2020-03-29T00:00:00Z   \n",
       "4  1244051647032102914   788863557349670913  2020-03-29T00:00:00Z   \n",
       "\n",
       "    screen_name                                               text     source  \\\n",
       "0  IMSS_SanLuis  Ante cualquier enfermedad respiratoria, no te ...  TweetDeck   \n",
       "1    intrac_ccs  #ATENCIÓN En el Terminal Nuevo Circo se implem...  TweetDeck   \n",
       "2      rlieving  “People are just storing up. They are staying ...  TweetDeck   \n",
       "3  Tu_IMSS_Coah  Si empezaste a trabajar, necesitas dar de alta...  TweetDeck   \n",
       "4  Tabasco_IMSS  Una sociedad informada está mejor preparada an...  TweetDeck   \n",
       "\n",
       "   reply_to_status_id  reply_to_user_id reply_to_screen_name  is_quote  ...  \\\n",
       "0                 NaN               NaN                  NaN     False  ...   \n",
       "1                 NaN               NaN                  NaN     False  ...   \n",
       "2                 NaN               NaN                  NaN     False  ...   \n",
       "3                 NaN               NaN                  NaN     False  ...   \n",
       "4                 NaN               NaN                  NaN     False  ...   \n",
       "\n",
       "   retweet_count  country_code  place_full_name place_type followers_count  \\\n",
       "0              0           NaN              NaN        NaN            1008   \n",
       "1              1           NaN              NaN        NaN              90   \n",
       "2              0           NaN              NaN        NaN             136   \n",
       "3              0           NaN              NaN        NaN            1549   \n",
       "4              0           NaN              NaN        NaN             868   \n",
       "\n",
       "  friends_count  account_lang    account_created_at  verified lang  \n",
       "0            41           NaN  2017-05-04T22:00:38Z     False   es  \n",
       "1           316           NaN  2019-05-08T01:21:16Z     False   es  \n",
       "2           457           NaN  2009-10-08T21:06:08Z     False   en  \n",
       "3           170           NaN  2017-01-05T18:17:00Z     False   es  \n",
       "4           125           NaN  2016-10-19T22:05:03Z     False   es  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check first 5 rows of df_tweets dataset\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14607013 entries, 0 to 355386\n",
      "Data columns (total 22 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   status_id             int64  \n",
      " 1   user_id               int64  \n",
      " 2   created_at            object \n",
      " 3   screen_name           object \n",
      " 4   text                  object \n",
      " 5   source                object \n",
      " 6   reply_to_status_id    float64\n",
      " 7   reply_to_user_id      float64\n",
      " 8   reply_to_screen_name  object \n",
      " 9   is_quote              bool   \n",
      " 10  is_retweet            bool   \n",
      " 11  favourites_count      int64  \n",
      " 12  retweet_count         int64  \n",
      " 13  country_code          object \n",
      " 14  place_full_name       object \n",
      " 15  place_type            object \n",
      " 16  followers_count       int64  \n",
      " 17  friends_count         int64  \n",
      " 18  account_lang          float64\n",
      " 19  account_created_at    object \n",
      " 20  verified              bool   \n",
      " 21  lang                  object \n",
      "dtypes: bool(3), float64(3), int64(6), object(10)\n",
      "memory usage: 2.2+ GB\n"
     ]
    }
   ],
   "source": [
    "#Get summary of df_tweets dataframe\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status_id                      0\n",
       "user_id                        0\n",
       "created_at                     0\n",
       "screen_name                    2\n",
       "text                           0\n",
       "source                        83\n",
       "reply_to_status_id      12861933\n",
       "reply_to_user_id        12495830\n",
       "reply_to_screen_name    12495830\n",
       "is_quote                       0\n",
       "is_retweet                     0\n",
       "favourites_count               0\n",
       "retweet_count                  0\n",
       "country_code            13950294\n",
       "place_full_name         13947685\n",
       "place_type              13947685\n",
       "followers_count                0\n",
       "friends_count                  0\n",
       "account_lang            14607013\n",
       "account_created_at             0\n",
       "verified                       0\n",
       "lang                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check sum of missing values in each column of df_tweets dataframe\n",
    "df_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Remove Redundant Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows in which text are not English\n",
    "df_tweets = df_tweets[df_tweets.lang == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns with missing percentage of 60% or more\n",
    "df_tweets = df_tweets.loc[:, df_tweets.isnull().mean() < .6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8133785 entries, 2 to 355386\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   created_at          object\n",
      " 1   screen_name         object\n",
      " 2   text                object\n",
      " 3   source              object\n",
      " 4   is_quote            bool  \n",
      " 5   is_retweet          bool  \n",
      " 6   favourites_count    int64 \n",
      " 7   retweet_count       int64 \n",
      " 8   followers_count     int64 \n",
      " 9   friends_count       int64 \n",
      " 10  account_created_at  object\n",
      " 11  verified            bool  \n",
      " 12  lang                object\n",
      "dtypes: bool(3), int64(4), object(6)\n",
      "memory usage: 705.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#Remove redundant columns \n",
    "df_tweets = df_tweets.drop(['status_id', 'user_id'], axis = 1)\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813378, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly drop additional 90% of rows  \n",
    "df_tweets = df_tweets.sample(frac=.1)\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest tweet:  2020-03-29 00:00:00\n",
      "Latest tweet:  2020-04-30 23:59:57 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert created_at and account_created_at variables from object to datetime64 \n",
    "df_tweets['created_at'] = df_tweets['created_at'].astype('datetime64')\n",
    "df_tweets['account_created_at'] = df_tweets['account_created_at'].astype('datetime64')\n",
    "# Check for earliest and latest tweet\n",
    "print(\"Earliest tweet: \", df_tweets['created_at'].min())\n",
    "print(\"Latest tweet: \", df_tweets['created_at'].max(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Twitter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for twitter text cleaning \n",
    "def tweet_cleaner(text):\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                  '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text) #remove url links\n",
    "    text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text) #remove twitter handles (@user)\n",
    "    # convert emojis to text\n",
    "    text = emojis.decode(text)\n",
    "    text = text.replace(\":\",\" \")\n",
    "    text = ' '.join(text.split())\n",
    "    # remove punctuations, numbers, and special characters \n",
    "    text = re.sub(\"[^a-zA-Z'_]\",\" \",text) #include contractions with \"'\" and emoji texts with \"_\"\n",
    "    # remove whitespaces \n",
    "    text = ' '.join(text.split()) \n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'demojize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2df9210fdd18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Apply text cleaning function to cleaned_text column in df_tweets dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-2df9210fdd18>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Apply text cleaning function to cleaned_text column in df_tweets dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-74d5e659fbbf>\u001b[0m in \u001b[0;36mtweet_cleaner\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(@[A-Za-z0-9_]+)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove twitter handles (@user)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# convert emojis to text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdemojize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'demojize'"
     ]
    }
   ],
   "source": [
    "#Apply text cleaning function to cleaned_text column in df_tweets dataframe\n",
    "df_tweets['cleaned_text'] = df_tweets['text'].apply(lambda x: tweet_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lemmatizing words\n",
    "def lemmatize_text(text):\n",
    "    wl = WordNetLemmatizer()\n",
    "    token_words=word_tokenize(str(text))\n",
    "    token_words\n",
    "    lemmatize_text=[]\n",
    "    for word in token_words:\n",
    "        lemmatize_text.append(wl.lemmatize(word))\n",
    "        lemmatize_text.append(\" \")\n",
    "    return \"\".join(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply lemmatizing text function to cleaned_text column in df_tweets dataframe\n",
    "df_tweets['cleaned_text'] = df_tweets['cleaned_text'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words in cleaned_text column\n",
    "\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stopword_text = [w for w in str(text).split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the remove stop words function to cleaned_text column in df_tweets dataframe\n",
    "df_tweets['cleaned_text'] = df_tweets['cleaned_text'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Final Dataset into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv (r'C:/Users/melin/Documents/Springboard Data Science Career Track/Capstone Projects/Capstone Project 2/df_tweets.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "\n",
    "# Binary Relevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('C:/Users/melin/Documents/Springboard Data Science Career Track/Capstone Projects/Capstone Project 2/df_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for labeling sentiment words\n",
    "def sentimentize(text):\n",
    "    if TextBlob(str(text)).sentiment.subjectivity <= 0.05:\n",
    "        label = 'neutral'\n",
    "    elif TextBlob(str(text)).sentiment.polarity > 0.1:\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'negative'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply sentimentize function to cleaned_text column in df_tweets dataframe\n",
    "df_tweets['sentiment'] = df_tweets['cleaned_text'].apply(lambda x: sentimentize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Counts of Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     315501\n",
       "negative    255354\n",
       "positive    242523\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the value counts of sentiment column in df_tweets\n",
    "df_tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Dataset with Sentiment Labels to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv (r'C:/Users/melin/Documents/Springboard Data Science Career Track/Capstone Projects/Capstone Project 2/tweets_sentiment.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows containing missing values under the cleaned_text column \n",
    "df_tweets = df_tweets[df_tweets['cleaned_text'].notnull()]\n",
    "\n",
    "##Format sentiment labels (target variable) for input into sklearn’s MultiLabelBinarizer( )\n",
    "#Converting values in target variable into lists\n",
    "y = df_tweets['sentiment'].str.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encode the target variable by using sklearn’s MultiLabelBinarizer( )\n",
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform target variable\n",
    "y = multilabel_binarizer.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and validation set\n",
    "xtrain, xval, ytrain, yval = train_test_split(df_tweets['cleaned_text'], y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using TF-IDF to extract features from the cleaned version of the text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000) # set 10,000 most frequent words in the data as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TF-IDF features\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
    "xval_tfidf = tfidf_vectorizer.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Naive Bayes Classifier for Multinomial Models with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sklearn’s OneVsRestClassifier class to solve the Naive Bayes Classifier model's problem as a Binary Relevance or one-vs-all problem\n",
    "nb = MultinomialNB()\n",
    "clf_nb = OneVsRestClassifier(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                            fit_prior=True),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model on TF-IDF train data\n",
    "clf_nb.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for validation set\n",
    "y_pred_tfidf_nb = clf_nb.predict(xval_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Linear Support Vector Machine with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sklearn’s OneVsRestClassifier class to solve the Linear Support Vector Machine model's problem as a Binary Relevance or one-vs-all problem\n",
    "lsvm = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, tol=None)\n",
    "clf_lsvm = OneVsRestClassifier(lsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=SGDClassifier(alpha=0.001, average=False,\n",
       "                                            class_weight=None,\n",
       "                                            early_stopping=False, epsilon=0.1,\n",
       "                                            eta0=0.0, fit_intercept=True,\n",
       "                                            l1_ratio=0.15,\n",
       "                                            learning_rate='optimal',\n",
       "                                            loss='hinge', max_iter=1000,\n",
       "                                            n_iter_no_change=5, n_jobs=None,\n",
       "                                            penalty='l2', power_t=0.5,\n",
       "                                            random_state=42, shuffle=True,\n",
       "                                            tol=None, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model on TF-IDF train data\n",
    "clf_lsvm.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for validation set\n",
    "y_pred_tfidf_lsvm = clf_lsvm.predict(xval_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Logistic Regression with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use sklearn’s OneVsRestClassifier class to solve the Logistic Regression model's problem as a Binary Relevance or one-vs-all problem\n",
    "lr = LogisticRegression(max_iter = 4000)\n",
    "clf_lr = OneVsRestClassifier(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=4000,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model on TF-IDF train data\n",
    "clf_lr.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions for validation set\n",
    "y_pred_tfidf_lr = clf_lr.predict(xval_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Naive Bayes Classifier for Multinomial Models with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[49999   322   836]\n",
      " [31499 30901   549]\n",
      " [28604    73 19892]]\n",
      "\n",
      "Accuracy: 0.45\n",
      "\n",
      "Micro Precision: 0.97\n",
      "Micro Recall: 0.47\n",
      "Micro F1-score: 0.63\n",
      "\n",
      "Macro Precision: 0.97\n",
      "Macro Recall: 0.47\n",
      "Macro F1-score: 0.63\n",
      "\n",
      "Weighted Precision: 0.97\n",
      "Weighted Recall: 0.47\n",
      "Weighted F1-score: 0.63\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.96      0.44      0.61     51157\n",
      "     neutral       0.99      0.49      0.66     62949\n",
      "    negative       0.93      0.41      0.57     48569\n",
      "\n",
      "   micro avg       0.96      0.45      0.62    162675\n",
      "   macro avg       0.96      0.45      0.61    162675\n",
      "weighted avg       0.96      0.45      0.61    162675\n",
      " samples avg       0.45      0.45      0.45    162675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Evaluate TF-IDF and Naive Bayes Classifier for Multinomial Models with OneVsRestClassifier\n",
    "\n",
    "#confusion matrix\n",
    "confusion = confusion_matrix(yval.argmax(axis=1), y_pred_tfidf_nb.argmax(axis=1))\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(yval, y_pred_tfidf_nb)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_nb, average='micro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_nb, average='micro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_nb, average='micro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_nb, average='macro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_nb, average='macro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_nb, average='macro', labels=np.unique(y_pred_tfidf_nb))))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_nb, average='weighted', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_nb, average='weighted', labels=np.unique(y_pred_tfidf_nb))))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(yval, y_pred_tfidf_nb, average='weighted', labels=np.unique(y_pred_tfidf_nb))))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(yval, y_pred_tfidf_nb, target_names=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Linear Support Vector Machine with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[50659   257   241]\n",
      " [56892  6056     1]\n",
      " [40072   155  8342]]\n",
      "\n",
      "Accuracy: 0.11\n",
      "\n",
      "Micro Precision: 0.94\n",
      "Micro Recall: 0.08\n",
      "Micro F1-score: 0.15\n",
      "\n",
      "Macro Precision: 0.94\n",
      "Macro Recall: 0.08\n",
      "Macro F1-score: 0.14\n",
      "\n",
      "Weighted Precision: 0.94\n",
      "Weighted Recall: 0.08\n",
      "Weighted F1-score: 0.15\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.94      0.06      0.11     51157\n",
      "     neutral       0.94      0.10      0.17     62949\n",
      "    negative       0.97      0.17      0.29     48569\n",
      "\n",
      "   micro avg       0.95      0.11      0.19    162675\n",
      "   macro avg       0.95      0.11      0.19    162675\n",
      "weighted avg       0.95      0.11      0.19    162675\n",
      " samples avg       0.11      0.11      0.11    162675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Evaluate TF-IDF and Linear Support Vector Machine with OneVsRestClassifier\n",
    "\n",
    "#confusion matrix\n",
    "confusion = confusion_matrix(yval.argmax(axis=1), y_pred_tfidf_lsvm.argmax(axis=1))\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(yval, y_pred_tfidf_lsvm)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lsvm, average='micro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lsvm, average='micro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_lsvm, average='micro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lsvm, average='macro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lsvm, average='macro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_lsvm, average='macro', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lsvm, average='weighted', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lsvm, average='weighted', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(yval, y_pred_tfidf_lsvm, average='weighted', labels=np.unique(y_pred_tfidf_lsvm))))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(yval, y_pred_tfidf_lsvm, target_names=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF and Logistic Regression with OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[50014   394   749]\n",
      " [  883 62032    34]\n",
      " [ 4440   205 43924]]\n",
      "\n",
      "Accuracy: 0.93\n",
      "\n",
      "Micro Precision: 0.97\n",
      "Micro Recall: 0.95\n",
      "Micro F1-score: 0.96\n",
      "\n",
      "Macro Precision: 0.97\n",
      "Macro Recall: 0.95\n",
      "Macro F1-score: 0.96\n",
      "\n",
      "Weighted Precision: 0.97\n",
      "Weighted Recall: 0.95\n",
      "Weighted F1-score: 0.96\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.94      0.91      0.93     51157\n",
      "     neutral       0.99      0.99      0.99     62949\n",
      "    negative       0.96      0.93      0.95     48569\n",
      "\n",
      "   micro avg       0.97      0.95      0.96    162675\n",
      "   macro avg       0.96      0.94      0.95    162675\n",
      "weighted avg       0.97      0.95      0.96    162675\n",
      " samples avg       0.94      0.95      0.94    162675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Evaluate TF-IDF and Logistic Regression with OneVsRestClassifier\n",
    "\n",
    "#confusion matrix\n",
    "confusion = confusion_matrix(yval.argmax(axis=1), y_pred_tfidf_lr.argmax(axis=1))\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(yval, y_pred_tfidf_lr)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lr, average='micro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lr, average='micro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_lr, average='micro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lr, average='macro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lr, average='macro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(yval, y_pred_tfidf_lr, average='macro', labels=np.unique(y_pred_tfidf_lr))))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(yval, y_pred_tfidf_lr, average='weighted', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(yval, y_pred_tfidf_lr, average='weighted', labels=np.unique(y_pred_tfidf_lr))))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(yval, y_pred_tfidf_lr, average='weighted', labels=np.unique(y_pred_tfidf_lr))))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(yval, y_pred_tfidf_lr, target_names=['positive', 'neutral', 'negative']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
